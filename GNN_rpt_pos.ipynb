{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "561e984b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:90% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>.output_result { max-width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>.prompt { min-width: 30ex !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import time\n",
    "import sys\n",
    "import itertools\n",
    "import random\n",
    "import os\n",
    "import signal\n",
    "import gc\n",
    "import psutil\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "def visualize(h, color):\n",
    "    z = TSNE(n_components=2).fit_transform(h.detach().cpu().numpy())\n",
    "\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "\n",
    "    plt.scatter(z[:, 0], z[:, 1], s=70, c=color, cmap=\"Set2\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "np.set_printoptions(threshold=sys.maxsize,linewidth=512)\n",
    "#np.set_printoptions(threshold=256)\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))\n",
    "display(HTML(\"<style>.output_result { max-width:95% !important; }</style>\"))\n",
    " \n",
    "#여백 줄이기\n",
    "display(HTML(\"<style>.prompt { min-width: 30ex !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab550bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import EarlyStopping\n",
    "# from pytorchtools import EarlyStopping\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=10, verbose=False, delta=0, path='checkpoint.pt'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): validation loss가 개선된 후 기다리는 기간\n",
    "                            Default: 10\n",
    "            verbose (bool): True일 경우 각 validation loss의 개선 사항 메세지 출력\n",
    "                            Default: False\n",
    "            delta (float): 개선되었다고 인정되는 monitered quantity의 최소 변화\n",
    "                            Default: 0\n",
    "            path (str): checkpoint저장 경로\n",
    "                            Default: 'checkpoint.pt'\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), self.path)\n",
    "        self.val_loss_min = val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53c9b18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_length=100\n",
    "unit_space_x=5\n",
    "unit_space_y=5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b3b80d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_normal_int(size):\n",
    "    sigma=size/4\n",
    "    m=size/2\n",
    "    min=1\n",
    "    max=size-1\n",
    "    a=0\n",
    "    while a <=0 or a>=size-1:\n",
    "        a=math.floor(sigma*np.random.randn()+m)\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc90ae00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_flat(active,y,x):\n",
    "    x_dim = active.shape[1]\n",
    "    y_dim = active.shape[0]\n",
    "    coord = y*x_dim + x\n",
    "    return coord\n",
    "\n",
    "def cal_unflat(active,num):\n",
    "    x_dim = active.shape[1]\n",
    "    y_dim = active.shape[0]\n",
    "    y = int(num / x_dim)\n",
    "    x = num % x_dim\n",
    "    return y,x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2a1ecaad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####tools for calculating distance to interconnect\n",
    "\n",
    "def find_start_end_mid(active_routing):\n",
    "    a1=np.where(active_routing==1)[1]\n",
    "    unique_elements, counts=np.unique(a1, return_counts=True)\n",
    "    mid=unique_elements[counts>1][0]\n",
    "    \n",
    "    y1=np.where(active_routing==2)[0][1]\n",
    "    y2=np.where(active_routing==2)[0][0]\n",
    "    x1=np.where(active_routing==2)[1][1]\n",
    "    x2=np.where(active_routing==2)[1][0]\n",
    "    start= np.array([[y1,x1]])\n",
    "    end = np.array([[y2,x2]])\n",
    "    mid1 = np.array([[y1,mid]])\n",
    "    mid2 = np.array([[y2,mid]])\n",
    "    return start,end,mid1,mid2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d58f4906",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_div_pos(divide,start,end,mid1,mid2):\n",
    "    div_pos=np.copy(start)\n",
    "    line1_length = (np.linalg.norm(start[0]-mid1[0]))*unit_space_x\n",
    "    line2_length = (np.linalg.norm(mid1[0]-mid2[0]))*unit_space_y\n",
    "    line3_length = (np.linalg.norm(mid2[0]-end[0]))*unit_space_x\n",
    "    line_num=np.array([])\n",
    "    for i in range(divide.shape[0]):\n",
    "            \n",
    "            if divide[i]<=line1_length :\n",
    "                temp2 = np.copy(start)\n",
    "                temp2[0][1] += divide[i]/unit_space_x\n",
    "                div_pos = np.append(div_pos,temp2,axis=0)\n",
    "                line_num=np.append(line_num,np.array([1]))\n",
    "            if line1_length<divide[i]<=line1_length+line2_length:\n",
    "                temp2 = np.copy(mid1)\n",
    "                temp2[0][0] -= (divide[i]-line1_length)/unit_space_y\n",
    "                div_pos = np.append(div_pos,temp2,axis=0)\n",
    "                line_num=np.append(line_num,np.array([2]))\n",
    "\n",
    "            if divide[i] > line1_length+line2_length:\n",
    "                temp2 = np.copy(mid2)\n",
    "                temp2[0][1] += (divide[i]-line1_length-line2_length)/unit_space_x\n",
    "                div_pos = np.append(div_pos,temp2,axis=0)\n",
    "                line_num=np.append(line_num,np.array([3]))\n",
    "\n",
    "                \n",
    "    div_pos = np.delete(div_pos,0,axis=0)\n",
    "    return div_pos,line_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2da7ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c4f1bd2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide_line(active,min_rpt,start,end,mid1,mid2,add_num=0):\n",
    "#    unit_space_x = int(target_x/active.shape[1])\n",
    "#    unit_space_y = int(target_y/active.shape[0])\n",
    "#    print(\"distance:\",distance)\n",
    "#    print(\"minimum # of repeaters:\",min_rpt)\n",
    "#    print(\"input:\",x1,y1)\n",
    "#    print(\"output:\",x2,y2)\n",
    "#    print(\"mid:\",mid)\n",
    "    min_rpt = min_rpt + add_num\n",
    "    distance = np.sum(abs(start-end)*unit_space_x)    \n",
    "    unit_distance = int(distance / (min_rpt+1) )\n",
    "    divide = np.array([])\n",
    "    divide_line = np.array([])\n",
    "    for i in range(min_rpt):\n",
    "        divide = np.append(divide, np.array(int(unit_distance/unit_space_x)*unit_space_x*(i+1)))\n",
    "        \n",
    "    div_pos,line_num = find_div_pos(divide,start,end,mid1,mid2)\n",
    "    \n",
    "    space=np.min(np.append(divide,distance)-np.append(0,divide))\n",
    "    bound= int((target_length-space)/unit_space_x)        \n",
    "    online_min=bound\n",
    "    for i in range(div_pos.shape[0]-1):\n",
    "        dist=int(math.dist(div_pos[i],div_pos[i+1]))\n",
    "        if dist<online_min:\n",
    "            online_min=dist\n",
    "    return divide,div_pos,line_num,online_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d34f28de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_node_features(active,active_inout,active_routing):\n",
    "    \n",
    "    start,end,mid1,mid2=find_start_end_mid(active_routing)\n",
    "#    divide,div_pos,line_num,online_min =divide_line(active,rpt_num,start,end,mid1,mid2,add_num=0)\n",
    "    ## create feature matrix (node_num x features) => In this case, ( 32x64, 6)\n",
    "    ## Features : Y-pos, X-pos, Active, Start/End, Interconnect, dist1, dist2, dist3, dist4\n",
    "    node_num=active.shape[0]*active.shape[1]\n",
    "    feature = np.zeros([node_num,9])\n",
    "#    print(feature.shape)    \n",
    "    ## insert 1st, 2nd features Y,X\n",
    "    for i in range(active.shape[0]):\n",
    "        for j in range(active.shape[1]):\n",
    "            order=cal_flat(active,i,j)\n",
    "            feature[order][1]= j\n",
    "            feature[order][0]= i\n",
    "\n",
    "    ## insert 3~5 feature Active / Start,End / Interconnect\n",
    "    active_all = active_inout.flatten()\n",
    "    active_all[active_all==0]= 1\n",
    "    active_all[active_all==-1]=0\n",
    "    active_all[active_all==2]=0\n",
    "    train_mask = active_all\n",
    "    test_mask = active_all\n",
    "    \n",
    "    active_all = active_inout.flatten()    \n",
    "    active_all[active_all==-1]=1\n",
    "    active_all[active_all==2]=1\n",
    "    feature[:,2]= active_all\n",
    "    \n",
    "    active_all = active_routing.flatten()\n",
    "    active_all[active_all==1]=0\n",
    "    active_all[active_all==2]=1\n",
    "    feature[:,3] = active_all\n",
    "    \n",
    "    active_all = active_routing.flatten()\n",
    "    active_all[active_all==2]=0\n",
    "    feature[:,4] = active_all\n",
    "    \n",
    "    start,end,mid1,mid2=find_start_end_mid(active_routing)\n",
    "\n",
    "    ## insert 5~8th feature distance from metal lines\n",
    "    dist1_y=abs(feature[:,0]-start[0][0])\n",
    "    dist1_x=abs(feature[:,1]-start[0][1])\n",
    "    dist2_y=abs(feature[:,0]-mid1[0][0])\n",
    "    dist2_x=abs(feature[:,1]-mid1[0][1])\n",
    "    dist3_y=abs(feature[:,0]-mid2[0][0])\n",
    "    dist3_x=abs(feature[:,1]-mid2[0][1])\n",
    "    dist4_y=abs(feature[:,0]-end[0][0])\n",
    "    dist4_x=abs(feature[:,1]-end[0][1])\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    feature[:,5]=dist1_y+dist1_x\n",
    "    feature[:,6]=dist2_y+dist2_x\n",
    "    feature[:,7]=dist3_y+dist3_x\n",
    "    feature[:,8]=dist4_y+dist4_x\n",
    "    \n",
    "    return feature, train_mask, test_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7ed052",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b587cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "active=r_input[1,:,:,0]\n",
    "active_inout = r_input[1,:,:,1]\n",
    "active_routing = r_input[1,:,:,2]\n",
    "\n",
    "rpt_num = np.argwhere(r_output_num==1)[:,1]+1\n",
    "\n",
    "feature,train_mask,test_mask = extract_node_features(active,active_inout,active_routing)\n",
    "start,end,mid1,mid2=find_start_end_mid(active_routing)\n",
    "divide,div_pos,line_num,online_min =divide_line(active,rpt_num[1],start,end,mid1,mid2,add_num=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed166733",
   "metadata": {},
   "outputs": [],
   "source": [
    "rpt_num.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4836299",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(start,end,mid1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "67ff4878",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def extract_edges(active,active_inout,active_routing,rpt_num):\n",
    "    start,end,mid1,mid2=find_start_end_mid(active_routing)\n",
    "    divide,div_pos,line_num,online_min =divide_line(active,rpt_num,start,end,mid1,mid2,add_num=0)\n",
    "\n",
    "\n",
    "    ###### Extract edges of connected lines\n",
    "    y=start[0][0]\n",
    "    x=start[0][1]\n",
    "\n",
    "    src_node=np.array([])\n",
    "    des_node=np.array([])\n",
    "    for i in range(10000):\n",
    "        x+=1\n",
    "        if x<active.shape[1] and active_routing[y,x]==1:\n",
    "            src_node=np.append(src_node,cal_flat(active,y,x-1))\n",
    "            des_node=np.append(des_node,cal_flat(active,y,x))\n",
    "        else:\n",
    "            x-=1\n",
    "            break\n",
    "\n",
    "    for i in range(10000):\n",
    "        y-=1\n",
    "        if (y>=0 and active_routing[y,x]==1):\n",
    "            src_node=np.append(src_node,cal_flat(active,y+1,x))\n",
    "            des_node=np.append(des_node,cal_flat(active,y,x))\n",
    "        else:\n",
    "            y+=1\n",
    "            break\n",
    "\n",
    "    for i in range(10000):\n",
    "        x+=1\n",
    "        if x<active.shape[1] and (active_routing[y,x] in [1,2]):\n",
    "            src_node=np.append(src_node,cal_flat(active,y,x-1))\n",
    "            des_node=np.append(des_node,cal_flat(active,y,x))\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    edge_weights = np.ones(src_node.shape[0])\n",
    "#    print(src_node.shape)\n",
    "#    print(des_node.shape)\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    ####### Extract edges of 0 active region to start,mid1,mid2,end\n",
    "#    for i in range(4):\n",
    "#        a=np.array(cal_flat(active,arr[0],arr[1]))\n",
    "#        src_node=np.append(src_node,a)\n",
    "#    for i in range(arr.shape[1]):\n",
    "#        des_node=np.append(des_node,cal_flat(active,start[0][0],start[0][1]))\n",
    "#    for i in range(arr.shape[1]):\n",
    "#        des_node=np.append(des_node,cal_flat(active,mid1[0][0],mid1[0][1]))\n",
    "#    for i in range(arr.shape[1]):\n",
    "#        des_node=np.append(des_node,cal_flat(active,mid2[0][0],mid2[0][1]))\n",
    "#    for i in range(arr.shape[1]):\n",
    "#        des_node=np.append(des_node,cal_flat(active,end[0][0],end[0][1]))\n",
    "\n",
    "#    start2=start.reshape(2,1)\n",
    "#    mid1_2=mid1.reshape(2,1)\n",
    "#    mid2_2=mid2.reshape(2,1)\n",
    "#    end2=end.reshape(2,1)\n",
    "#    edge_weights=np.append(edge_weights, np.sqrt(np.sum((arr-start2)**2,axis=0)))\n",
    "#    edge_weights=np.append(edge_weights, np.sqrt(np.sum((arr-mid1_2)**2,axis=0)))\n",
    "#    edge_weights=np.append(edge_weights, np.sqrt(np.sum((arr-mid2_2)**2,axis=0)))\n",
    "#    edge_weights=np.append(edge_weights, np.sqrt(np.sum((arr-end2)**2,axis=0)))\n",
    "#    edge_weights=edge_weights.reshape(-1,1)\n",
    "#    print(edge_weights.shape)\n",
    "\n",
    "    \n",
    "    ########Extract edges < div_pos  -  Active 0 > ######\n",
    "    arr=np.array(np.where(active_inout == 0))\n",
    "\n",
    "    for i in range(div_pos.shape[0]):\n",
    "        \n",
    "        src_node=np.append(src_node,cal_flat(active,arr[0],arr[1]))\n",
    "        des_node=np.append(des_node,cal_flat(active,div_pos[i][0],div_pos[i][1]).repeat(arr.shape[1]))\n",
    "        dist_pos=np.sqrt(np.sum((arr-div_pos[i].reshape(2,1))**2,axis=0))\n",
    "        dist_pos[dist_pos==0]=1\n",
    "        dist_pos = 1/dist_pos\n",
    "        edge_weights = np.append(edge_weights,dist_pos)\n",
    "\n",
    "#    print(src_node.shape)\n",
    "#    print(des_node.shape)    \n",
    "#    print(edge_weights.shape)\n",
    "\n",
    "    \n",
    "    edges=np.append(src_node,des_node).reshape(2,-1)\n",
    "    \n",
    "    return edges,edge_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ee5c94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c7f1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de8451e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc40abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### output for gnn ###\n",
    "r_output_modify = np.copy(r_input[:,:,:,0])\n",
    "print(r_output_modify.shape)\n",
    "\n",
    "for i in range(r_output_modify.shape[0]):\n",
    "    r_output_modify[i][r_output_modify[i]==-1]=3\n",
    "    coord=np.argwhere(r_output[i]==1)\n",
    "    for j in range(coord.shape[0]):\n",
    "        r_output_modify[i,coord[j][0],coord[j][1]]=4\n",
    "print(r_output_modify.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "11433192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 32, 64, 3)\n",
      "(10000, 32, 64, 3)\n",
      "(10000, 32, 64)\n",
      "(10000, 10)\n",
      "(10000, 2048)\n"
     ]
    }
   ],
   "source": [
    "r_input=np.load(\"./gnn/dataset3/input500k.npy\")[:10000]\n",
    "r_input_pos = np.load(\"./gnn/dataset3/input500k_pos.npy\")[:10000]\n",
    "r_output=np.load(\"./gnn/dataset3/output500k.npy\")[:10000]\n",
    "r_output_num = np.load(\"./gnn/dataset3/output500k_num.npy\")[:10000]\n",
    "r_output_pos = np.load(\"./gnn/dataset3/output500k_pos_flatten.npy\")[:10000]\n",
    "print(r_input.shape)\n",
    "print(r_input_pos.shape)\n",
    "print(r_output.shape)\n",
    "print(r_output_num.shape)\n",
    "print(r_output_pos.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b399570b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#np.save(\"./gnn/dataset3/output500k_pos.npy\",r_output_modify.astype(np.int8))\n",
    "print(r_output_num[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0f9bb6e6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from torch_geometric.data import Data\n",
    "import torch\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "def gen_graph(r_input,r_output_pos,r_output_num):\n",
    "    rpt_num = np.argwhere(r_output_num==1)[:,1]+1\n",
    "    \n",
    "    dataset=[]\n",
    "    for i in range(r_input.shape[0]):\n",
    "\n",
    "        active=r_input[i,:,:,0]\n",
    "        active_inout=r_input[i,:,:,1]\n",
    "        active_routing=r_input[i,:,:,2]\n",
    "        node_feature,train_mask,test_mask=extract_node_features(active,active_inout,active_routing)\n",
    "        edges,edge_weights=extract_edges(active,active_inout,active_routing,rpt_num[i])\n",
    "        \n",
    "        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        node_feature = torch.from_numpy(node_feature).to(device)\n",
    "        edges= torch.from_numpy(edges).to(device)\n",
    "        edge_weights = torch.from_numpy(edge_weights).to(device)\n",
    "        r_output_modify = torch.from_numpy(r_output_pos[i]).to(device)\n",
    "\n",
    "        node_feature = node_feature.to(dtype=torch.float32)\n",
    "        edges = edges.to(dtype=torch.long)\n",
    "        edge_weights = edge_weights.to(dtype=torch.float32)\n",
    "        r_output_modify = r_output_modify.to(dtype=torch.long)\n",
    "        \n",
    "#        r_output_modify = torch.tensor(np.sum(r_output[i]==1)).to(dtype=torch.long).to(device)\n",
    "        data=Data(\n",
    "            x=node_feature,\n",
    "            edge_index=edges,\n",
    "            y=r_output_modify,\n",
    "            edge_weight=edge_weights,\n",
    "            train_mask=train_mask,\n",
    "            test_mask=test_mask,\n",
    "        )\n",
    "#        data.num_classes=len(torch.unique(data.y))\n",
    "        data.num_classes=2       \n",
    "#        data.edge_weight = edge_weights\n",
    "#        data.train_mask = train_mask\n",
    "#        data.test_mask = test_mask\n",
    "        dataset.append(data)\n",
    "    \n",
    "    return dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eedf4c1a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GCN(\n",
       "  (conv): ModuleList(\n",
       "    (0): GCNConv(9, 64)\n",
       "    (1): GCNConv(64, 64)\n",
       "    (2): GCNConv(64, 64)\n",
       "    (3): GCNConv(64, 64)\n",
       "    (4): GCNConv(64, 64)\n",
       "    (5): GCNConv(64, 64)\n",
       "    (6): GCNConv(64, 64)\n",
       "    (7): GCNConv(64, 64)\n",
       "    (8): GCNConv(64, 64)\n",
       "    (9): GCNConv(64, 64)\n",
       "    (10): GCNConv(64, 64)\n",
       "  )\n",
       "  (out): Linear(in_features=64, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv,GATConv,GATv2Conv,SAGEConv\n",
    "from torch_geometric.nn import global_mean_pool as gap, global_max_pool as gmp\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GCN, self).__init__()\n",
    "        torch.manual_seed(44)\n",
    "        \n",
    "        self.conv = torch.nn.ModuleList()\n",
    "        self.conv.append(GCNConv(9, 64))\n",
    "        for i in range(10):\n",
    "            self.conv.append(GCNConv(64,64))\n",
    "        \n",
    "        self.out = Linear(64, 1)\n",
    "        \n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index,edge_weights = data.x, data.edge_index, data.edge_weight\n",
    "        \n",
    "        for conv in self.conv:\n",
    "            x = conv(x, edge_index, edge_weight=edge_weights)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, p=0.5, training = self.training)\n",
    "        \n",
    "#        x = F.softmax(self.out(x),dim=1)\n",
    "        x= torch.sigmoid(self.out(x))\n",
    "#        x = torch.cat([gmp(x, batch_index)])\n",
    "        \n",
    "#        return F.softmax(x,dim=1)\n",
    "        x= x.squeeze()\n",
    "        return x\n",
    "\n",
    "#edge_weight=edge_weights\n",
    "model = GCN()\n",
    "\n",
    "model = model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d824e147",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6051d482",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "def weighted_binary_crossentropy(y_true, y_pred):\n",
    "    epsilon = 1e-7\n",
    "    \n",
    "#    y_true = torch.reshape(torch.tensor(y_true), [-1])\n",
    "#    y_pred = torch.reshape(torch.tensor(y_pred), [-1])\n",
    "    y_true_np = y_true.clone().detach().cpu().numpy().astype('int32')\n",
    "    y_true_float = y_true.float().requires_grad_(True)\n",
    "    y_pred_float = y_pred.float().requires_grad_(True)\n",
    "    class_weights = torch.tensor(class_weight.compute_class_weight(class_weight='balanced', classes=[0, 1], y=y_true_np), dtype=torch.float32)\n",
    "    y_pred_float = torch.clamp(y_pred_float, epsilon, 1 - epsilon)\n",
    "    loss = -y_true_float * torch.log(y_pred_float) * class_weights[1] - (1 - y_true_float) * torch.log(1 - y_pred_float) * class_weights[0]\n",
    "    \n",
    "    # Alternatively, you can use PyTorch's built-in binary cross-entropy loss with sample weights\n",
    "    # bce = F.binary_cross_entropy(y_pred_float, y_true_float, weight=class_weights)\n",
    "    # loss = torch.mean(bce)\n",
    "    \n",
    "    return torch.mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ef9d5634",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pytorchtools import EarlyStopping\n",
    "#early_stopping = EarlyStopping(patience=10, verbose=True, delta=0.001)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, weight_decay=5e-4)\n",
    "#criterion = torch.nn.CrossEntropyLoss()\n",
    "#criterion = torch.nn.BCELoss()\n",
    "\n",
    "def train(dataset):\n",
    "    \n",
    "    model.train()\n",
    "    optimizer.zero_grad()  # Clear gradients.\n",
    "    out = model(dataset)  # Perform a single forward pass.\n",
    "#    loss = criterion(out[dataset.train_mask].reshape(-1), dataset.y[dataset.train_mask].float())  # Compute the loss solely based on the training nodes.\n",
    "    loss = weighted_binary_crossentropy(dataset.y, out)\n",
    "    loss.backward()  # Derive gradients.\n",
    "    optimizer.step()  # Update parameters based on gradients.\n",
    "    return loss\n",
    "\n",
    "\n",
    "def test(dataset):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0.0\n",
    "        val_acc = 0.0\n",
    "        out2 = model(dataset)\n",
    "        val_loss = weighted_binary_crossentropy(dataset.y,out2)\n",
    "    return val_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249b3c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "model = GCN()\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a0e58a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "dataset=gen_graph(r_input,r_output.reshape(r_output.shape[0],-1),r_output_num)\n",
    "len(dataset)\n",
    "\n",
    "batchs = 64\n",
    "epochs = 10\n",
    "\n",
    "early_stopping = EarlyStopping(patience = 20, verbose = True)\n",
    "\n",
    "# Define the sizes of the training and validation (or test) sets\n",
    "train_size = int(0.9 * len(dataset)) # 90% of the dataset will be used for training\n",
    "val_size = len(dataset) - train_size # the remaining 20% of the dataset will be used for validation\n",
    "\n",
    "# Use the random_split function to split the dataset\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# Define the data loaders for the training and validation (or test) sets\n",
    "train_loader = DataLoader(train_dataset, batch_size=batchs, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batchs, shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "#loader=DataLoader(dataset, batch_size =batchs, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b3f20f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8ffdc57b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 0.7655, Val_Loss: 0.6936\n",
      "Epoch: 002, Loss: 0.6905, Val_Loss: 0.6928\n",
      "Epoch: 003, Loss: 0.6896, Val_Loss: 0.6927\n",
      "Epoch: 004, Loss: 0.6204, Val_Loss: 0.6929\n",
      "Epoch: 005, Loss: 0.5995, Val_Loss: 0.6933\n",
      "Epoch: 006, Loss: 0.6225, Val_Loss: 0.6940\n",
      "Epoch: 007, Loss: 0.5876, Val_Loss: 0.6951\n",
      "Epoch: 008, Loss: 0.5903, Val_Loss: 0.6964\n",
      "Epoch: 009, Loss: 0.5781, Val_Loss: 0.6981\n",
      "Epoch: 010, Loss: 0.5631, Val_Loss: 0.7001\n",
      "Epoch: 011, Loss: 0.5677, Val_Loss: 0.7027\n",
      "Epoch: 012, Loss: 0.5919, Val_Loss: 0.7060\n",
      "Epoch: 013, Loss: 0.5615, Val_Loss: 0.7102\n",
      "Epoch: 014, Loss: 0.5545, Val_Loss: 0.7155\n",
      "Epoch: 015, Loss: 0.5999, Val_Loss: 0.7225\n",
      "Epoch: 016, Loss: 0.5378, Val_Loss: 0.7316\n",
      "Epoch: 017, Loss: 0.4858, Val_Loss: 0.7436\n",
      "Epoch: 018, Loss: 0.5152, Val_Loss: 0.7585\n",
      "Epoch: 019, Loss: 0.4942, Val_Loss: 0.7774\n",
      "Epoch: 020, Loss: 0.5113, Val_Loss: 0.8002\n",
      "Epoch: 021, Loss: 0.5458, Val_Loss: 0.8266\n",
      "Epoch: 022, Loss: 0.4933, Val_Loss: 0.8576\n",
      "Epoch: 023, Loss: 0.5260, Val_Loss: 0.8952\n",
      "Epoch: 024, Loss: 0.4661, Val_Loss: 0.9364\n",
      "Epoch: 025, Loss: 0.4134, Val_Loss: 0.9795\n",
      "Epoch: 026, Loss: 0.4138, Val_Loss: 1.0235\n",
      "Epoch: 027, Loss: 0.4731, Val_Loss: 1.0698\n",
      "Epoch: 028, Loss: 0.4358, Val_Loss: 1.1194\n",
      "Epoch: 029, Loss: 0.4206, Val_Loss: 1.1662\n",
      "Epoch: 030, Loss: 0.3754, Val_Loss: 1.2180\n",
      "Epoch: 031, Loss: 0.4031, Val_Loss: 1.2735\n",
      "Epoch: 032, Loss: 0.4020, Val_Loss: 1.3216\n",
      "Epoch: 033, Loss: 0.3504, Val_Loss: 1.3719\n",
      "Epoch: 034, Loss: 0.4559, Val_Loss: 1.4252\n",
      "Epoch: 035, Loss: 0.4002, Val_Loss: 1.4701\n",
      "Epoch: 036, Loss: 0.3752, Val_Loss: 1.5241\n",
      "Epoch: 037, Loss: 0.4252, Val_Loss: 1.5668\n",
      "Epoch: 038, Loss: 0.3776, Val_Loss: 1.6122\n",
      "Epoch: 039, Loss: 0.3486, Val_Loss: 1.6476\n",
      "Epoch: 040, Loss: 0.3984, Val_Loss: 1.6772\n",
      "Epoch: 041, Loss: 0.4086, Val_Loss: 1.7091\n",
      "Epoch: 042, Loss: 0.3999, Val_Loss: 1.7441\n",
      "Epoch: 043, Loss: 0.3732, Val_Loss: 1.7744\n",
      "Epoch: 044, Loss: 0.3852, Val_Loss: 1.7994\n",
      "Epoch: 045, Loss: 0.3967, Val_Loss: 1.8314\n",
      "Epoch: 046, Loss: 0.4030, Val_Loss: 1.8571\n",
      "Epoch: 047, Loss: 0.4400, Val_Loss: 1.8690\n",
      "Epoch: 048, Loss: 0.3996, Val_Loss: 1.8782\n",
      "Epoch: 049, Loss: 0.4024, Val_Loss: 1.8939\n",
      "Epoch: 050, Loss: 0.4018, Val_Loss: 1.9151\n",
      "Epoch: 051, Loss: 0.3652, Val_Loss: 1.9246\n",
      "Epoch: 052, Loss: 0.3603, Val_Loss: 1.9235\n",
      "Epoch: 053, Loss: 0.4309, Val_Loss: 1.9263\n",
      "Epoch: 054, Loss: 0.4081, Val_Loss: 1.9308\n",
      "Epoch: 055, Loss: 0.3986, Val_Loss: 1.9361\n",
      "Epoch: 056, Loss: 0.3710, Val_Loss: 1.9365\n",
      "Epoch: 057, Loss: 0.3363, Val_Loss: 1.9450\n",
      "Epoch: 058, Loss: 0.3561, Val_Loss: 1.9529\n",
      "Epoch: 059, Loss: 0.4108, Val_Loss: 1.9607\n",
      "Epoch: 060, Loss: 0.3347, Val_Loss: 1.9480\n",
      "Epoch: 061, Loss: 0.4265, Val_Loss: 1.9523\n",
      "Epoch: 062, Loss: 0.3854, Val_Loss: 1.9495\n",
      "Epoch: 063, Loss: 0.4502, Val_Loss: 1.9584\n",
      "Epoch: 064, Loss: 0.3625, Val_Loss: 1.9655\n",
      "Epoch: 065, Loss: 0.3836, Val_Loss: 1.9499\n",
      "Epoch: 066, Loss: 0.3967, Val_Loss: 1.9460\n",
      "Epoch: 067, Loss: 0.4230, Val_Loss: 1.9514\n",
      "Epoch: 068, Loss: 0.4075, Val_Loss: 1.9439\n",
      "Epoch: 069, Loss: 0.3851, Val_Loss: 1.9297\n",
      "Epoch: 070, Loss: 0.4147, Val_Loss: 1.9306\n",
      "Epoch: 071, Loss: 0.4077, Val_Loss: 1.9244\n",
      "Epoch: 072, Loss: 0.3968, Val_Loss: 1.9304\n",
      "Epoch: 073, Loss: 0.3967, Val_Loss: 1.9235\n",
      "Epoch: 074, Loss: 0.4160, Val_Loss: 1.9170\n",
      "Epoch: 075, Loss: 0.3849, Val_Loss: 1.9135\n",
      "Epoch: 076, Loss: 0.3765, Val_Loss: 1.9153\n",
      "Epoch: 077, Loss: 0.3490, Val_Loss: 1.9136\n",
      "Epoch: 078, Loss: 0.3999, Val_Loss: 1.9057\n",
      "Epoch: 079, Loss: 0.3857, Val_Loss: 1.9053\n",
      "Epoch: 080, Loss: 0.3046, Val_Loss: 1.8896\n",
      "Epoch: 081, Loss: 0.3661, Val_Loss: 1.8901\n",
      "Epoch: 082, Loss: 0.3664, Val_Loss: 1.8851\n",
      "Epoch: 083, Loss: 0.3214, Val_Loss: 1.8898\n",
      "Epoch: 084, Loss: 0.3324, Val_Loss: 1.8676\n",
      "Epoch: 085, Loss: 0.3763, Val_Loss: 1.8682\n",
      "Epoch: 086, Loss: 0.3698, Val_Loss: 1.8635\n",
      "Epoch: 087, Loss: 0.3746, Val_Loss: 1.8532\n",
      "Epoch: 088, Loss: 0.3347, Val_Loss: 1.8602\n",
      "Epoch: 089, Loss: 0.3622, Val_Loss: 1.8498\n",
      "Epoch: 090, Loss: 0.3797, Val_Loss: 1.8430\n",
      "Epoch: 091, Loss: 0.3409, Val_Loss: 1.8256\n",
      "Epoch: 092, Loss: 0.3411, Val_Loss: 1.8201\n",
      "Epoch: 093, Loss: 0.3328, Val_Loss: 1.8228\n",
      "Epoch: 094, Loss: 0.3312, Val_Loss: 1.8053\n",
      "Epoch: 095, Loss: 0.3868, Val_Loss: 1.7922\n",
      "Epoch: 096, Loss: 0.3362, Val_Loss: 1.7997\n",
      "Epoch: 097, Loss: 0.3458, Val_Loss: 1.7744\n",
      "Epoch: 098, Loss: 0.4146, Val_Loss: 1.7712\n",
      "Epoch: 099, Loss: 0.3107, Val_Loss: 1.7587\n",
      "Epoch: 100, Loss: 0.3292, Val_Loss: 1.7441\n",
      "Epoch: 101, Loss: 0.2819, Val_Loss: 1.7284\n",
      "Epoch: 102, Loss: 0.2807, Val_Loss: 1.7162\n",
      "Epoch: 103, Loss: 0.3047, Val_Loss: 1.7183\n",
      "Epoch: 104, Loss: 0.2911, Val_Loss: 1.7048\n",
      "Epoch: 105, Loss: 0.3365, Val_Loss: 1.6828\n",
      "Epoch: 106, Loss: 0.2993, Val_Loss: 1.6730\n",
      "Epoch: 107, Loss: 0.2762, Val_Loss: 1.6628\n",
      "Epoch: 108, Loss: 0.3604, Val_Loss: 1.6481\n",
      "Epoch: 109, Loss: 0.3348, Val_Loss: 1.6558\n",
      "Epoch: 110, Loss: 0.2964, Val_Loss: 1.6557\n",
      "Epoch: 111, Loss: 0.3166, Val_Loss: 1.6631\n",
      "Epoch: 112, Loss: 0.2892, Val_Loss: 1.6637\n",
      "Epoch: 113, Loss: 0.2641, Val_Loss: 1.6625\n",
      "Epoch: 114, Loss: 0.2663, Val_Loss: 1.6767\n",
      "Epoch: 115, Loss: 0.3094, Val_Loss: 1.6740\n",
      "Epoch: 116, Loss: 0.2934, Val_Loss: 1.6867\n",
      "Epoch: 117, Loss: 0.2703, Val_Loss: 1.6857\n",
      "Epoch: 118, Loss: 0.2561, Val_Loss: 1.6976\n",
      "Epoch: 119, Loss: 0.2826, Val_Loss: 1.7025\n",
      "Epoch: 120, Loss: 0.3195, Val_Loss: 1.7132\n",
      "Epoch: 121, Loss: 0.3262, Val_Loss: 1.7319\n",
      "Epoch: 122, Loss: 0.2744, Val_Loss: 1.7400\n",
      "Epoch: 123, Loss: 0.3062, Val_Loss: 1.7400\n",
      "Epoch: 124, Loss: 0.2733, Val_Loss: 1.7488\n",
      "Epoch: 125, Loss: 0.2483, Val_Loss: 1.7515\n",
      "Epoch: 126, Loss: 0.3243, Val_Loss: 1.7842\n",
      "Epoch: 127, Loss: 0.2830, Val_Loss: 1.7617\n",
      "Epoch: 128, Loss: 0.2676, Val_Loss: 1.7768\n",
      "Epoch: 129, Loss: 0.2472, Val_Loss: 1.7652\n",
      "Epoch: 130, Loss: 0.2602, Val_Loss: 1.7644\n",
      "Epoch: 131, Loss: 0.2560, Val_Loss: 1.7827\n",
      "Epoch: 132, Loss: 0.2896, Val_Loss: 1.7784\n",
      "Epoch: 133, Loss: 0.2461, Val_Loss: 1.7789\n",
      "Epoch: 134, Loss: 0.2717, Val_Loss: 1.7925\n",
      "Epoch: 135, Loss: 0.2522, Val_Loss: 1.7933\n",
      "Epoch: 136, Loss: 0.2419, Val_Loss: 1.7766\n",
      "Epoch: 137, Loss: 0.2494, Val_Loss: 1.7591\n",
      "Epoch: 138, Loss: 0.2667, Val_Loss: 1.7813\n",
      "Epoch: 139, Loss: 0.2517, Val_Loss: 1.7598\n",
      "Epoch: 140, Loss: 0.2836, Val_Loss: 1.7763\n",
      "Epoch: 141, Loss: 0.2414, Val_Loss: 1.7700\n",
      "Epoch: 142, Loss: 0.2752, Val_Loss: 1.7502\n",
      "Epoch: 143, Loss: 0.2884, Val_Loss: 1.7788\n",
      "Epoch: 144, Loss: 0.2671, Val_Loss: 1.7651\n",
      "Epoch: 145, Loss: 0.2565, Val_Loss: 1.7640\n",
      "Epoch: 146, Loss: 0.2610, Val_Loss: 1.7792\n",
      "Epoch: 147, Loss: 0.2486, Val_Loss: 1.7942\n",
      "Epoch: 148, Loss: 0.2413, Val_Loss: 1.7760\n",
      "Epoch: 149, Loss: 0.2565, Val_Loss: 1.7915\n",
      "Epoch: 150, Loss: 0.2415, Val_Loss: 1.7757\n",
      "Epoch: 151, Loss: 0.2309, Val_Loss: 1.7310\n",
      "Epoch: 152, Loss: 0.2688, Val_Loss: 1.8093\n",
      "Epoch: 153, Loss: 0.2414, Val_Loss: 1.7887\n",
      "Epoch: 154, Loss: 0.2458, Val_Loss: 1.7900\n",
      "Epoch: 155, Loss: 0.2427, Val_Loss: 1.7886\n",
      "Epoch: 156, Loss: 0.2539, Val_Loss: 1.8163\n",
      "Epoch: 157, Loss: 0.2417, Val_Loss: 1.7784\n",
      "Epoch: 158, Loss: 0.2474, Val_Loss: 1.7513\n",
      "Epoch: 159, Loss: 0.2404, Val_Loss: 1.7677\n",
      "Epoch: 160, Loss: 0.2336, Val_Loss: 1.7783\n",
      "Epoch: 161, Loss: 0.2549, Val_Loss: 1.8157\n",
      "Epoch: 162, Loss: 0.2788, Val_Loss: 1.8286\n",
      "Epoch: 163, Loss: 0.2300, Val_Loss: 1.8127\n",
      "Epoch: 164, Loss: 0.2578, Val_Loss: 1.8020\n",
      "Epoch: 165, Loss: 0.2773, Val_Loss: 1.8055\n",
      "Epoch: 166, Loss: 0.2576, Val_Loss: 1.8334\n",
      "Epoch: 167, Loss: 0.2482, Val_Loss: 1.8326\n",
      "Epoch: 168, Loss: 0.2791, Val_Loss: 1.8162\n",
      "Epoch: 169, Loss: 0.2421, Val_Loss: 1.8559\n",
      "Epoch: 170, Loss: 0.2123, Val_Loss: 1.8457\n",
      "Epoch: 171, Loss: 0.2651, Val_Loss: 1.8620\n",
      "Epoch: 172, Loss: 0.2368, Val_Loss: 1.8648\n",
      "Epoch: 173, Loss: 0.2317, Val_Loss: 1.8732\n",
      "Epoch: 174, Loss: 0.2424, Val_Loss: 1.8644\n",
      "Epoch: 175, Loss: 0.2402, Val_Loss: 1.8735\n",
      "Epoch: 176, Loss: 0.2413, Val_Loss: 1.8846\n",
      "Epoch: 177, Loss: 0.2487, Val_Loss: 1.8467\n",
      "Epoch: 178, Loss: 0.2385, Val_Loss: 1.8754\n",
      "Epoch: 179, Loss: 0.2410, Val_Loss: 1.9113\n",
      "Epoch: 180, Loss: 0.2460, Val_Loss: 1.8635\n",
      "Epoch: 181, Loss: 0.2317, Val_Loss: 1.8787\n",
      "Epoch: 182, Loss: 0.2524, Val_Loss: 1.8480\n",
      "Epoch: 183, Loss: 0.2361, Val_Loss: 1.8530\n",
      "Epoch: 184, Loss: 0.2547, Val_Loss: 1.8950\n",
      "Epoch: 185, Loss: 0.2224, Val_Loss: 1.8857\n",
      "Epoch: 186, Loss: 0.2315, Val_Loss: 1.8860\n",
      "Epoch: 187, Loss: 0.2675, Val_Loss: 1.8938\n",
      "Epoch: 188, Loss: 0.2573, Val_Loss: 1.9021\n",
      "Epoch: 189, Loss: 0.2483, Val_Loss: 1.8800\n",
      "Epoch: 190, Loss: 0.2569, Val_Loss: 1.8985\n",
      "Epoch: 191, Loss: 0.2347, Val_Loss: 1.8982\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 192, Loss: 0.2555, Val_Loss: 1.9051\n",
      "Epoch: 193, Loss: 0.3033, Val_Loss: 1.8956\n",
      "Epoch: 194, Loss: 0.2189, Val_Loss: 1.9241\n",
      "Epoch: 195, Loss: 0.2843, Val_Loss: 1.9290\n",
      "Epoch: 196, Loss: 0.2431, Val_Loss: 1.9271\n",
      "Epoch: 197, Loss: 0.2205, Val_Loss: 1.9178\n",
      "Epoch: 198, Loss: 0.2576, Val_Loss: 1.9203\n",
      "Epoch: 199, Loss: 0.2266, Val_Loss: 1.9107\n",
      "Epoch: 200, Loss: 0.2633, Val_Loss: 1.9273\n",
      "Epoch: 201, Loss: 0.2577, Val_Loss: 1.9515\n",
      "Epoch: 202, Loss: 0.2709, Val_Loss: 1.9579\n",
      "Epoch: 203, Loss: 0.2392, Val_Loss: 1.9832\n",
      "Epoch: 204, Loss: 0.2486, Val_Loss: 1.9639\n",
      "Epoch: 205, Loss: 0.2369, Val_Loss: 1.9785\n",
      "Epoch: 206, Loss: 0.2447, Val_Loss: 1.9853\n",
      "Epoch: 207, Loss: 0.2608, Val_Loss: 1.9925\n",
      "Epoch: 208, Loss: 0.2561, Val_Loss: 2.0375\n",
      "Epoch: 209, Loss: 0.2670, Val_Loss: 2.0646\n",
      "Epoch: 210, Loss: 0.2179, Val_Loss: 2.0626\n",
      "Epoch: 211, Loss: 0.2417, Val_Loss: 2.0616\n",
      "Epoch: 212, Loss: 0.2838, Val_Loss: 2.0656\n",
      "Epoch: 213, Loss: 0.2431, Val_Loss: 2.1099\n",
      "Epoch: 214, Loss: 0.2358, Val_Loss: 2.1728\n",
      "Epoch: 215, Loss: 0.2967, Val_Loss: 2.1475\n",
      "Epoch: 216, Loss: 0.2140, Val_Loss: 2.1681\n",
      "Epoch: 217, Loss: 0.2501, Val_Loss: 2.2262\n",
      "Epoch: 218, Loss: 0.2328, Val_Loss: 2.2263\n",
      "Epoch: 219, Loss: 0.2155, Val_Loss: 2.2624\n",
      "Epoch: 220, Loss: 0.2774, Val_Loss: 2.2712\n",
      "Epoch: 221, Loss: 0.2231, Val_Loss: 2.2680\n",
      "Epoch: 222, Loss: 0.2106, Val_Loss: 2.2801\n",
      "Epoch: 223, Loss: 0.2370, Val_Loss: 2.3386\n",
      "Epoch: 224, Loss: 0.2526, Val_Loss: 2.2948\n",
      "Epoch: 225, Loss: 0.2623, Val_Loss: 2.3657\n",
      "Epoch: 226, Loss: 0.2256, Val_Loss: 2.3734\n",
      "Epoch: 227, Loss: 0.2628, Val_Loss: 2.3652\n",
      "Epoch: 228, Loss: 0.2310, Val_Loss: 2.3981\n",
      "Epoch: 229, Loss: 0.2058, Val_Loss: 2.4152\n",
      "Epoch: 230, Loss: 0.2465, Val_Loss: 2.4554\n",
      "Epoch: 231, Loss: 0.2221, Val_Loss: 2.5017\n",
      "Epoch: 232, Loss: 0.2183, Val_Loss: 2.5827\n",
      "Epoch: 233, Loss: 0.2363, Val_Loss: 2.5466\n",
      "Epoch: 234, Loss: 0.2231, Val_Loss: 2.5977\n",
      "Epoch: 235, Loss: 0.2399, Val_Loss: 2.6099\n",
      "Epoch: 236, Loss: 0.2336, Val_Loss: 2.6076\n",
      "Epoch: 237, Loss: 0.2538, Val_Loss: 2.5753\n",
      "Epoch: 238, Loss: 0.2605, Val_Loss: 2.6215\n",
      "Epoch: 239, Loss: 0.2571, Val_Loss: 2.6251\n",
      "Epoch: 240, Loss: 0.2189, Val_Loss: 2.5588\n",
      "Epoch: 241, Loss: 0.2173, Val_Loss: 2.6445\n",
      "Epoch: 242, Loss: 0.2326, Val_Loss: 2.6366\n",
      "Epoch: 243, Loss: 0.2178, Val_Loss: 2.6589\n",
      "Epoch: 244, Loss: 0.2261, Val_Loss: 2.6926\n",
      "Epoch: 245, Loss: 0.2237, Val_Loss: 2.7406\n",
      "Epoch: 246, Loss: 0.2380, Val_Loss: 2.7303\n",
      "Epoch: 247, Loss: 0.2374, Val_Loss: 2.7030\n",
      "Epoch: 248, Loss: 0.2654, Val_Loss: 2.6875\n",
      "Epoch: 249, Loss: 0.2512, Val_Loss: 2.7433\n",
      "Epoch: 250, Loss: 0.2458, Val_Loss: 2.6786\n",
      "Epoch: 251, Loss: 0.2396, Val_Loss: 2.8278\n",
      "Epoch: 252, Loss: 0.2391, Val_Loss: 2.7625\n",
      "Epoch: 253, Loss: 0.2340, Val_Loss: 2.7253\n",
      "Epoch: 254, Loss: 0.2116, Val_Loss: 2.7300\n",
      "Epoch: 255, Loss: 0.2175, Val_Loss: 2.7422\n",
      "Epoch: 256, Loss: 0.2501, Val_Loss: 2.8366\n",
      "Epoch: 257, Loss: 0.2383, Val_Loss: 2.8911\n",
      "Epoch: 258, Loss: 0.2304, Val_Loss: 2.7960\n",
      "Epoch: 259, Loss: 0.2370, Val_Loss: 2.8288\n",
      "Epoch: 260, Loss: 0.2242, Val_Loss: 2.8395\n",
      "Epoch: 261, Loss: 0.2501, Val_Loss: 2.8874\n",
      "Epoch: 262, Loss: 0.2348, Val_Loss: 2.9346\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m500\u001b[39m):\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m----> 5\u001b[0m         loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m#        losses.append(loss)\u001b[39;00m\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch2 \u001b[38;5;129;01min\u001b[39;00m val_loader:\n",
      "Cell \u001b[1;32mIn[15], line 12\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(dataset)\u001b[0m\n\u001b[0;32m     10\u001b[0m     model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m     11\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()  \u001b[38;5;66;03m# Clear gradients.\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Perform a single forward pass.\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m#    loss = criterion(out[dataset.train_mask].reshape(-1), dataset.y[dataset.train_mask].float())  # Compute the loss solely based on the training nodes.\u001b[39;00m\n\u001b[0;32m     14\u001b[0m     loss \u001b[38;5;241m=\u001b[39m weighted_binary_crossentropy(dataset\u001b[38;5;241m.\u001b[39my, out)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\rl\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[13], line 27\u001b[0m, in \u001b[0;36mGCN.forward\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m     24\u001b[0m x, edge_index,edge_weights \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mx, data\u001b[38;5;241m.\u001b[39medge_index, data\u001b[38;5;241m.\u001b[39medge_weight\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m conv \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv:\n\u001b[1;32m---> 27\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43medge_weights\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(x)\n\u001b[0;32m     29\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mdropout(x, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m, training \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\rl\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\rl\\lib\\site-packages\\torch_geometric\\nn\\conv\\gcn_conv.py:197\u001b[0m, in \u001b[0;36mGCNConv.forward\u001b[1;34m(self, x, edge_index, edge_weight)\u001b[0m\n\u001b[0;32m    194\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlin(x)\n\u001b[0;32m    196\u001b[0m \u001b[38;5;66;03m# propagate_type: (x: Tensor, edge_weight: OptTensor)\u001b[39;00m\n\u001b[1;32m--> 197\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpropagate\u001b[49m\u001b[43m(\u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43medge_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    198\u001b[0m \u001b[43m                     \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    200\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    201\u001b[0m     out \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\rl\\lib\\site-packages\\torch_geometric\\nn\\conv\\message_passing.py:391\u001b[0m, in \u001b[0;36mMessagePassing.propagate\u001b[1;34m(self, edge_index, size, **kwargs)\u001b[0m\n\u001b[0;32m    388\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m res \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    389\u001b[0m         aggr_kwargs \u001b[38;5;241m=\u001b[39m res[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(res, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m res\n\u001b[1;32m--> 391\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maggregate(out, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39maggr_kwargs)\n\u001b[0;32m    393\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_aggregate_forward_hooks\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[0;32m    394\u001b[0m     res \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, (aggr_kwargs, ), out)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\rl\\lib\\site-packages\\torch_geometric\\nn\\conv\\message_passing.py:514\u001b[0m, in \u001b[0;36mMessagePassing.aggregate\u001b[1;34m(self, inputs, index, ptr, dim_size)\u001b[0m\n\u001b[0;32m    501\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21maggregate\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs: Tensor, index: Tensor,\n\u001b[0;32m    502\u001b[0m               ptr: Optional[Tensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    503\u001b[0m               dim_size: Optional[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m    504\u001b[0m     \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Aggregates messages from neighbors as\u001b[39;00m\n\u001b[0;32m    505\u001b[0m \u001b[38;5;124;03m    :math:`\\square_{j \\in \\mathcal{N}(i)}`.\u001b[39;00m\n\u001b[0;32m    506\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;124;03m    as specified in :meth:`__init__` by the :obj:`aggr` argument.\u001b[39;00m\n\u001b[0;32m    513\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maggr_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mptr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mptr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdim_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    515\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnode_dim\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\rl\\lib\\site-packages\\torch_geometric\\nn\\aggr\\base.py:114\u001b[0m, in \u001b[0;36mAggregation.__call__\u001b[1;34m(self, x, index, ptr, dim_size, dim, **kwargs)\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m index\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m dim_size \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(index\u001b[38;5;241m.\u001b[39mmax()):\n\u001b[0;32m    110\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEncountered invalid \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdim_size\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m (got \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    111\u001b[0m                          \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdim_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m but expected \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    112\u001b[0m                          \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m>= \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mint\u001b[39m(index\u001b[38;5;241m.\u001b[39mmax()) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(x, index, ptr, dim_size, dim, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\rl\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\rl\\lib\\site-packages\\torch_geometric\\nn\\aggr\\basic.py:21\u001b[0m, in \u001b[0;36mSumAggregation.forward\u001b[1;34m(self, x, index, ptr, dim_size, dim)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor, index: Optional[Tensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     19\u001b[0m             ptr: Optional[Tensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, dim_size: Optional[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     20\u001b[0m             dim: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m---> 21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mptr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduce\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msum\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\rl\\lib\\site-packages\\torch_geometric\\nn\\aggr\\base.py:153\u001b[0m, in \u001b[0;36mAggregation.reduce\u001b[1;34m(self, x, index, ptr, dim_size, dim, reduce)\u001b[0m\n\u001b[0;32m    150\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m segment_csr(x, ptr, reduce\u001b[38;5;241m=\u001b[39mreduce)\n\u001b[0;32m    152\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 153\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mscatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdim_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduce\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreduce\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\rl\\lib\\site-packages\\torch_geometric\\utils\\scatter.py:64\u001b[0m, in \u001b[0;36mscatter\u001b[1;34m(src, index, dim, out, dim_size, reduce)\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mscatter\u001b[39m(src: Tensor, index: Tensor, dim: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m     62\u001b[0m             out: Optional[Tensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, dim_size: Optional[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     63\u001b[0m             reduce: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msum\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m---> 64\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch_scatter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduce\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\rl\\lib\\site-packages\\torch_scatter\\scatter.py:152\u001b[0m, in \u001b[0;36mscatter\u001b[1;34m(src, index, dim, out, dim_size, reduce)\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;124;03m|\u001b[39;00m\n\u001b[0;32m     80\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;124;03m    torch.Size([10, 3, 64])\u001b[39;00m\n\u001b[0;32m    150\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reduce \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msum\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124madd\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m--> 152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mscatter_sum\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reduce \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmul\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m scatter_mul(src, index, dim, out, dim_size)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\rl\\lib\\site-packages\\torch_scatter\\scatter.py:20\u001b[0m, in \u001b[0;36mscatter_sum\u001b[1;34m(src, index, dim, out, dim_size)\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     19\u001b[0m         size[dim] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(index\u001b[38;5;241m.\u001b[39mmax()) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 20\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msrc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msrc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\u001b[38;5;241m.\u001b[39mscatter_add_(dim, index, src)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_loss=0\n",
    "val_loss=0\n",
    "for epoch in range(1, 500):\n",
    "    for batch in train_loader:\n",
    "  \n",
    "        loss = train(batch)\n",
    "        train_loss += loss\n",
    "        \n",
    "    for batch2 in val_loader:\n",
    "        val_loss = test(batch2)\n",
    "        val_losses += val_loss\n",
    "    train_losses /= len(train_loader)\n",
    "    val_losses /= len(val_loader)\n",
    "    \n",
    "    if epoch % 1 == 0:\n",
    "        print(f'Epoch: {epoch:03d}, Loss: {train_losses:.4f}, Val_Loss: {val_losses:.4f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "39c47d75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 1 0 0 1 0 0 0 0 1 1 0 0 0 0 1 1 0 1 1 1 1 1 1 0 1 0 0 1 1 1 0 1 0 0 1 1 0 0 0 0 0 0 1 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "out=model(dataset[0]).cpu().detach().numpy()\n",
    "threshold=0.95\n",
    "out = np.where(out>threshold, 1,0).reshape(32,64)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf640fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(r_input[0,:,:,2])\n",
    "print(r_output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ec7fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(r_output_pos[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f73f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384e7223",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a70927d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_example(y,x,num=1):\n",
    "    start_time = time.time()\n",
    "    input=np.zeros([1,y,x,3])\n",
    "    output=np.zeros([1,y,x])\n",
    "    \n",
    "\n",
    "    while num > 0 :\n",
    "        add_num=0\n",
    "        error=0\n",
    "        seed=active_seed((y,x))\n",
    "        active_origin=active_trans(seed,2,4)\n",
    "        active,active_inout,active_routing,mid,distance,min_rpt,x1,x2,y1,y2=in_out_mid_gen(active_origin)\n",
    "        start,end,mid1,mid2,line1,line2,line3=define_interconnect(mid,x1,x2,y1,y2)\n",
    "            \n",
    "        for i in range(5):\n",
    "            divide, div_pos,line_num,online_min= divide_line(active,min_rpt,start,end,mid1,mid2,add_num)\n",
    "            add_num+=1\n",
    "            opt_pos,pos,rpt_dist,error=find_opt_delay(active,active_inout,divide,div_pos,online_min)\n",
    "            if error==1:\n",
    "                break\n",
    "            dist=cal_rpt_dist(active,opt_pos,pos,start,end,rpt_dist)\n",
    "            if np.max(dist)<=100:\n",
    "                num -= 1\n",
    "                out=np.zeros(active.shape).astype(np.int8)\n",
    "                for i in range(opt_pos.shape[0]):\n",
    "\n",
    "                    xx=opt_pos[i][0]\n",
    "                    yy=opt_pos[i][1]\n",
    "                    out[yy][xx]=1\n",
    "                out=out.reshape([1,y,x])\n",
    "\n",
    "                temp=np.zeros([1,y,x,3])\n",
    "\n",
    "                temp[0,:,:,0] = active.reshape(1,active.shape[0],active.shape[1])\n",
    "                temp[0,:,:,1] = active_inout.reshape(1,active.shape[0],active.shape[1])\n",
    "                temp[0,:,:,2] = active_routing.reshape(1,active.shape[0],active.shape[1])\n",
    "                input=np.append(input,temp,axis=0)\n",
    "                output = np.append(output,out,axis=0)\n",
    "#                print(\"Add_num:::::\",add_num)\n",
    "                break\n",
    "            if i == 5:\n",
    "                if np.max(dist)>100:\n",
    "                    error =1 \n",
    "                    print(\"######################ERROR###############\")\n",
    "                \n",
    "\n",
    "    output = np.delete(output,0,axis=0).astype(np.int8)\n",
    "    input = np.delete(input,0,axis=0).astype(np.int8)\n",
    "    np.save(\"./gnn/dataset/input.npy\",input)\n",
    "    np.save(\"./gnn/dataset/output.npy\",output)\n",
    "    end_time = time.time()\n",
    "    print(\"Total time:\",int(end_time-start_time),\"s\")\n",
    "    return input,output,dist\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee86981e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "input,output,dist=gen_example(32,64,1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714c3b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(input.dtype)\n",
    "a=np.array([0],dtype=int)\n",
    "print(a.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b24fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "b=np.copy(input[0][:,:,2])\n",
    "a=np.where(output[0]==1)\n",
    "print(a[0])\n",
    "print(a[1])\n",
    "for i in range(a[0].shape[0]):\n",
    "    yy=a[0][i]\n",
    "    xx=a[1][i]\n",
    "    b[yy][xx]=3\n",
    "print(b)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb49a0b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#np.save(\"./dataset2/1000/input100k.npy\",r_input)\n",
    "#np.save(\"./dataset2/1000/output100k.npy\",r_output)\n",
    "r_input=np.load(\"./dataset2/1000/input100k.npy\")\n",
    "r_output=np.load(\"./dataset2/1000/output100k.npy\")\n",
    "print(r_input.shape)\n",
    "print(r_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fcacbf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tt=np.zeros([100000,16384]).astype(np.int8)\n",
    "for i in range(tt.shape[0]):\n",
    "    tt[i]=r_output[i].flatten()\n",
    "\n",
    "print(tt.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5262d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#resnet50 = tf.keras.applications.resnet.ResNet50(weights='imagenet', input_shape = (128,128,3),include_top=False)\n",
    "#resnet50.summary()\n",
    "\n",
    "eff_net=tf.keras.applications.EfficientNetV2S(weights=\"imagenet\",input_shape=(128,128,3),include_top=False)\n",
    "eff_net.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c979065",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model3 = tf.keras.Sequential([\n",
    "    eff_net,\n",
    "    \n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(128,activation='sigmoid')\n",
    "])\n",
    "\n",
    "#model3.compile(optimizer='Adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model3.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e167d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=30)\n",
    "save = tf.keras.callbacks.ModelCheckpoint('best_model_2D.h5', monitor='val_loss', mode='min', save_best_only=True)\n",
    "callback = [es, save]\n",
    "\n",
    "model3.fit(r_input,tt2, epochs=200, batch_size = 128, validation_split =0.3, callbacks=callback )    \n",
    "model3.save(\"./weights_resnet_2D.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc410ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model3\n",
    "for i in range(20):\n",
    "    gc.collect()\n",
    "    K.clear_session()\n",
    "    tf.keras.backend.clear_session()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef149b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model3 = tf.keras.models.load_model ('./weights_resnet.h5')\n",
    "#model3 = tf.keras.models.load_model ('./best_model.h5')\n",
    "\n",
    "pred=model3.predict(r_input)\n",
    "total = np.zeros(pred.shape[0])\n",
    "ans=np.where(pred>0.4,1,0)\n",
    "true=0\n",
    "false=0\n",
    "for i in range(ans.shape[0]):\n",
    "    verify1=np.isclose(tt2[i],ans[i],atol=0.1)\n",
    "    verify2=np.where(verify1==False)[0]\n",
    "    if verify2.shape==(0,):\n",
    "        total[i]=True\n",
    "        true += 1\n",
    "    else :\n",
    "        total[i]=False\n",
    "        false += 1\n",
    "print(\"True:\",true)\n",
    "print(\"Flase:\",false)\n",
    "print(\"Accuracy:\",(true/(true+false))*100,\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a711b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pred[5].reshape(64,64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c71c8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify(model,threshold=0.4):\n",
    "    true=0\n",
    "    false=0\n",
    "    input,output,cal=gen_example(64,64,bound=5,num=100)\n",
    "    out=np.zeros([100,4096]).astype(np.int8)\n",
    "    for i in range(output.shape[0]):\n",
    "        out[i]=output[i].flatten()\n",
    "    pred=model.predict(input)\n",
    "    pred=np.where(pred > threshold,1,0)\n",
    "    false_all = np.zeros(output.shape[0])\n",
    "    \n",
    "    for i in range(pred.shape[0]):\n",
    "        verify1=np.isclose(out[i],pred[i],atol=0.1)\n",
    "        verify2=np.where(verify1 == False)[0]\n",
    "        if verify2.shape == (0,):\n",
    "            total[i]=True\n",
    "            true += 1\n",
    "        else :\n",
    "            total[i]=False\n",
    "            false += 1\n",
    "            false_all[i]=1\n",
    "    #torch.cuda.empty_cache()\n",
    "    \n",
    "    return true, false, false_all, input, output, pred    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6b3297",
   "metadata": {},
   "outputs": [],
   "source": [
    "true, false, false_all, test, test_out, pred = verify(model3, 0.35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07be0f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"True:\",true)\n",
    "print(\"False:\",false)\n",
    "print(\"Accuracy:\", (true/(true+false))*100,\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e11420",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test[60][:,:,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59d8438",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03cb8a02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3764ba3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ac2b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(active_origin[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72aefd69",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
